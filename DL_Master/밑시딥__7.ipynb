{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWupMZocMxESj8IUeGW9FH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/comandi1969/AIFFEL_Online_Quest/blob/main/DL_Master/%EB%B0%91%EC%8B%9C%EB%94%A5__7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3-sBjxKxdgOJ"
      },
      "outputs": [],
      "source": [
        "class Pooling:\n",
        "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
        "        self.pool_h = pool_h\n",
        "        self.pool_w = pool_w\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "        self.x = None\n",
        "        self.arg_max = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
        "\n",
        "        arg_max = np.argmax(col, axis=1)\n",
        "        out = np.max(col, axis=1)\n",
        "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.arg_max = arg_max\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# simple_convnet.py\n",
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import pickle\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "from common.layers import *\n",
        "from common.gradient import numerical_gradient\n",
        "\n",
        "\n",
        "class SimpleConvNet:\n",
        "    \"\"\"단순한 합성곱 신경망\n",
        "\n",
        "    conv - relu - pool - affine - relu - affine - softmax\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
        "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
        "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
        "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
        "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
        "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
        "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=(1, 28, 28),\n",
        "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
        "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
        "        filter_num = conv_param['filter_num']\n",
        "        filter_size = conv_param['filter_size']\n",
        "        filter_pad = conv_param['pad']\n",
        "        filter_stride = conv_param['stride']\n",
        "        input_size = input_dim[1]\n",
        "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
        "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
        "\n",
        "        # 가중치 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * \\\n",
        "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
        "        self.params['b1'] = np.zeros(filter_num)\n",
        "        self.params['W2'] = weight_init_std * \\\n",
        "                            np.random.randn(pool_output_size, hidden_size)\n",
        "        self.params['b2'] = np.zeros(hidden_size)\n",
        "        self.params['W3'] = weight_init_std * \\\n",
        "                            np.random.randn(hidden_size, output_size)\n",
        "        self.params['b3'] = np.zeros(output_size)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
        "                                           conv_param['stride'], conv_param['pad'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
        "        self.layers['Relu2'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
        "\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        \"\"\"손실 함수를 구한다.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        \"\"\"\n",
        "        y = self.predict(x)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t, batch_size=100):\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "        acc = 0.0\n",
        "\n",
        "        for i in range(int(x.shape[0] / batch_size)):\n",
        "            tx = x[i*batch_size:(i+1)*batch_size]\n",
        "            tt = t[i*batch_size:(i+1)*batch_size]\n",
        "            y = self.predict(tx)\n",
        "            y = np.argmax(y, axis=1)\n",
        "            acc += np.sum(y == tt)\n",
        "\n",
        "        return acc / x.shape[0]\n",
        "\n",
        "    def numerical_gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다（수치미분）.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        loss_w = lambda w: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        for idx in (1, 2, 3):\n",
        "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
        "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다(오차역전파법).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def save_params(self, file_name=\"params.pkl\"):\n",
        "        params = {}\n",
        "        for key, val in self.params.items():\n",
        "            params[key] = val\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=\"params.pkl\"):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "        for key, val in params.items():\n",
        "            self.params[key] = val\n",
        "\n",
        "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
        "            self.layers[key].W = self.params['W' + str(i+1)]\n",
        "            self.layers[key].b = self.params['b' + str(i+1)]\n"
      ],
      "metadata": {
        "id": "9ZN5kHycfzjk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "# sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from common.simple_convnet import SimpleConvNet\n",
        "from common.trainer import Trainer\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
        "x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28),\n",
        "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "# 매개변수 보존\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "# 그래프 그리기\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "efvJXbpkgc0i",
        "outputId": "95d99458-e36d-4985-cc9b-07c6ae650163"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Creating pickle file ...\n",
            "Done!\n",
            "train loss:2.299164265574655\n",
            "=== epoch:1, train acc:0.097, test acc:0.108 ===\n",
            "train loss:2.297713748840736\n",
            "train loss:2.29208491914448\n",
            "train loss:2.2900162033294573\n",
            "train loss:2.282172721567297\n",
            "train loss:2.276190883202125\n",
            "train loss:2.256179402979977\n",
            "train loss:2.237156904840324\n",
            "train loss:2.24242977557541\n",
            "train loss:2.2073342941635867\n",
            "train loss:2.163202204334097\n",
            "train loss:2.141978323494877\n",
            "train loss:2.0843026282310153\n",
            "train loss:2.077733777731855\n",
            "train loss:2.0655070765705275\n",
            "train loss:1.9496852041843604\n",
            "train loss:1.911019301353272\n",
            "train loss:1.8627634921317233\n",
            "train loss:1.7459304229835806\n",
            "train loss:1.6741361176601068\n",
            "train loss:1.6105182485286114\n",
            "train loss:1.48449990551203\n",
            "train loss:1.4953013697322484\n",
            "train loss:1.2875290200729474\n",
            "train loss:1.2610156645920285\n",
            "train loss:1.297741497135118\n",
            "train loss:1.2320333660741694\n",
            "train loss:1.0144283446022277\n",
            "train loss:1.1471800197306896\n",
            "train loss:0.9094373667137654\n",
            "train loss:0.9127664087255141\n",
            "train loss:1.0064975505519682\n",
            "train loss:0.8778934384118419\n",
            "train loss:0.9438418192872455\n",
            "train loss:0.8378567043666435\n",
            "train loss:0.7510621491647685\n",
            "train loss:0.6428774225507625\n",
            "train loss:0.6360017572568478\n",
            "train loss:0.567259654355547\n",
            "train loss:0.5940877620701214\n",
            "train loss:0.6131522676645232\n",
            "train loss:0.46286431556892543\n",
            "train loss:0.7285044371122819\n",
            "train loss:0.5300356211086009\n",
            "train loss:0.5533580619144116\n",
            "train loss:0.6171994321766401\n",
            "train loss:0.7093928784121138\n",
            "train loss:0.44581332250268946\n",
            "train loss:0.6153559991671781\n",
            "train loss:0.6365316793485336\n",
            "train loss:0.6149356242868662\n",
            "=== epoch:2, train acc:0.809, test acc:0.804 ===\n",
            "train loss:0.37135269912244906\n",
            "train loss:0.5375722592117034\n",
            "train loss:0.4167172699344232\n",
            "train loss:0.4140176782893961\n",
            "train loss:0.4303440985360945\n",
            "train loss:0.5701375632795356\n",
            "train loss:0.42233134285113466\n",
            "train loss:0.41580970470310064\n",
            "train loss:0.5041036989783535\n",
            "train loss:0.43963984853851046\n",
            "train loss:0.345090161722645\n",
            "train loss:0.3794678787279716\n",
            "train loss:0.4111363213837372\n",
            "train loss:0.43386226313898973\n",
            "train loss:0.6616862501463145\n",
            "train loss:0.4772023609940541\n",
            "train loss:0.37708057409662116\n",
            "train loss:0.4357262579334099\n",
            "train loss:0.6171933992298458\n",
            "train loss:0.4622880180078846\n",
            "train loss:0.3337793009788155\n",
            "train loss:0.5930277104244368\n",
            "train loss:0.43458378683152477\n",
            "train loss:0.4721426641585623\n",
            "train loss:0.45996549144160404\n",
            "train loss:0.3842979640767454\n",
            "train loss:0.31403116880315196\n",
            "train loss:0.49914677799981355\n",
            "train loss:0.5067277428251699\n",
            "train loss:0.38003563568212334\n",
            "train loss:0.3367514361967073\n",
            "train loss:0.44854386606176755\n",
            "train loss:0.3844644820909735\n",
            "train loss:0.5288420844387253\n",
            "train loss:0.24705798110011074\n",
            "train loss:0.4781857604775087\n",
            "train loss:0.40246656254957747\n",
            "train loss:0.433023085009765\n",
            "train loss:0.287797784809427\n",
            "train loss:0.45332620028684917\n",
            "train loss:0.41319025095287626\n",
            "train loss:0.3547777599800079\n",
            "train loss:0.30719656491593667\n",
            "train loss:0.38387334079642044\n",
            "train loss:0.3277710154256227\n",
            "train loss:0.30436495118222195\n",
            "train loss:0.4017724562027709\n",
            "train loss:0.23731362536072811\n",
            "train loss:0.37277507602002835\n",
            "train loss:0.34819217746722836\n",
            "=== epoch:3, train acc:0.878, test acc:0.866 ===\n",
            "train loss:0.33318559840956496\n",
            "train loss:0.2987741747438582\n",
            "train loss:0.27443365076848764\n",
            "train loss:0.30382451461055976\n",
            "train loss:0.3187760358847775\n",
            "train loss:0.3740151922423135\n",
            "train loss:0.38048644099517104\n",
            "train loss:0.3803253963528398\n",
            "train loss:0.3072070871220606\n",
            "train loss:0.43928250780257605\n",
            "train loss:0.27125540544870236\n",
            "train loss:0.4890687633790645\n",
            "train loss:0.2786945363580958\n",
            "train loss:0.4349917281258638\n",
            "train loss:0.2414208601016478\n",
            "train loss:0.37720474654483843\n",
            "train loss:0.477540823661122\n",
            "train loss:0.26551998556092693\n",
            "train loss:0.2559090565878445\n",
            "train loss:0.3039519449556633\n",
            "train loss:0.3580195539018864\n",
            "train loss:0.4984285410862283\n",
            "train loss:0.2822582451529209\n",
            "train loss:0.4430815222632623\n",
            "train loss:0.3219405349073252\n",
            "train loss:0.28039176765915297\n",
            "train loss:0.22293736408773324\n",
            "train loss:0.2399353551998859\n",
            "train loss:0.24659968735815813\n",
            "train loss:0.3829704303047315\n",
            "train loss:0.3600426295515167\n",
            "train loss:0.27837162813855676\n",
            "train loss:0.293146767195687\n",
            "train loss:0.33786720714081364\n",
            "train loss:0.31899017390310513\n",
            "train loss:0.32383318075331724\n",
            "train loss:0.35769317897443104\n",
            "train loss:0.26631390130997684\n",
            "train loss:0.26769360720533814\n",
            "train loss:0.19580703149802056\n",
            "train loss:0.32532780622134494\n",
            "train loss:0.27050965945457484\n",
            "train loss:0.28563354436867305\n",
            "train loss:0.21349757219418525\n",
            "train loss:0.3378405239749911\n",
            "train loss:0.1921708879217109\n",
            "train loss:0.2647066555255197\n",
            "train loss:0.17360248577202783\n",
            "train loss:0.21787509527736718\n",
            "train loss:0.43242119500745607\n",
            "=== epoch:4, train acc:0.889, test acc:0.893 ===\n",
            "train loss:0.22824945510351394\n",
            "train loss:0.38851678309978327\n",
            "train loss:0.2784532050906429\n",
            "train loss:0.306054617235037\n",
            "train loss:0.3065229286666785\n",
            "train loss:0.1900562695582882\n",
            "train loss:0.43051328210442485\n",
            "train loss:0.18140158021032518\n",
            "train loss:0.5322881807107722\n",
            "train loss:0.24172811303287114\n",
            "train loss:0.1537259506321374\n",
            "train loss:0.2938691029038027\n",
            "train loss:0.37579654482113767\n",
            "train loss:0.29059666374924586\n",
            "train loss:0.11646593327318161\n",
            "train loss:0.307389473183264\n",
            "train loss:0.25075344397667404\n",
            "train loss:0.21557137624600106\n",
            "train loss:0.2664128364829473\n",
            "train loss:0.2881968356668772\n",
            "train loss:0.3433176135179838\n",
            "train loss:0.18665603687963275\n",
            "train loss:0.4188036208258727\n",
            "train loss:0.208401901897765\n",
            "train loss:0.21083088032202524\n",
            "train loss:0.2005863315581964\n",
            "train loss:0.13948064276516603\n",
            "train loss:0.29598714994959885\n",
            "train loss:0.13287552946978384\n",
            "train loss:0.377472816184669\n",
            "train loss:0.1080925003563668\n",
            "train loss:0.11303155924963287\n",
            "train loss:0.1566095695466661\n",
            "train loss:0.21050524380330085\n",
            "train loss:0.17952479412940137\n",
            "train loss:0.13340583789781157\n",
            "train loss:0.11210621201513976\n",
            "train loss:0.1752377986551589\n",
            "train loss:0.26978040286527133\n",
            "train loss:0.2784656624800423\n",
            "train loss:0.3306899697106263\n",
            "train loss:0.21254585710995116\n",
            "train loss:0.361410410695901\n",
            "train loss:0.1297034746643924\n",
            "train loss:0.21766676261204845\n",
            "train loss:0.17721872959748897\n",
            "train loss:0.2413029092898112\n",
            "train loss:0.18808098098319267\n",
            "train loss:0.08480949168496829\n",
            "train loss:0.2638004435430389\n",
            "=== epoch:5, train acc:0.91, test acc:0.886 ===\n",
            "train loss:0.34138439318722036\n",
            "train loss:0.6086006347375296\n",
            "train loss:0.19812719138129775\n",
            "train loss:0.3077746164296448\n",
            "train loss:0.2867980953946997\n",
            "train loss:0.2649024645886539\n",
            "train loss:0.270547140771244\n",
            "train loss:0.17838602057335712\n",
            "train loss:0.17427669899873802\n",
            "train loss:0.2427559118650873\n",
            "train loss:0.25550786913933965\n",
            "train loss:0.21893387467395273\n",
            "train loss:0.33049977489772475\n",
            "train loss:0.29676597137421085\n",
            "train loss:0.2460455406207419\n",
            "train loss:0.2096047257986282\n",
            "train loss:0.2692729289267637\n",
            "train loss:0.231090026091786\n",
            "train loss:0.17592734531584006\n",
            "train loss:0.15984654447373273\n",
            "train loss:0.30167344650323524\n",
            "train loss:0.32189080669322495\n",
            "train loss:0.18960755936843604\n",
            "train loss:0.19310362955079202\n",
            "train loss:0.16443240025120665\n",
            "train loss:0.2388818515183348\n",
            "train loss:0.39041423692083915\n",
            "train loss:0.20404013177245087\n",
            "train loss:0.21639502956286413\n",
            "train loss:0.13541400763332684\n",
            "train loss:0.26398445062748754\n",
            "train loss:0.18186868284913305\n",
            "train loss:0.22771642722335314\n",
            "train loss:0.1898633732749878\n",
            "train loss:0.23733293695264146\n",
            "train loss:0.18695096166719705\n",
            "train loss:0.12827194085243732\n",
            "train loss:0.20312204718233245\n",
            "train loss:0.19202058515941645\n",
            "train loss:0.17205215853593422\n",
            "train loss:0.22031675505564136\n",
            "train loss:0.15003590305415743\n",
            "train loss:0.2283822763320716\n",
            "train loss:0.2385775641569076\n",
            "train loss:0.3022261848628801\n",
            "train loss:0.1867769512221296\n",
            "train loss:0.3331620679997704\n",
            "train loss:0.1867921976678038\n",
            "train loss:0.14768253267930287\n",
            "train loss:0.1823947865086102\n",
            "=== epoch:6, train acc:0.917, test acc:0.915 ===\n",
            "train loss:0.24945194358394565\n",
            "train loss:0.16370772454493537\n",
            "train loss:0.2716815617484236\n",
            "train loss:0.1443678441230146\n",
            "train loss:0.3237624458801585\n",
            "train loss:0.12514514299623677\n",
            "train loss:0.19277032125024948\n",
            "train loss:0.1999442864443891\n",
            "train loss:0.2158516458101346\n",
            "train loss:0.23438167913819566\n",
            "train loss:0.25415960611345473\n",
            "train loss:0.35264201869177386\n",
            "train loss:0.15092291991977821\n",
            "train loss:0.3747256860252602\n",
            "train loss:0.1938870485868482\n",
            "train loss:0.1676471640988696\n",
            "train loss:0.19194035510303975\n",
            "train loss:0.18802502563609141\n",
            "train loss:0.22742260860637273\n",
            "train loss:0.17229364579771997\n",
            "train loss:0.0777381865274938\n",
            "train loss:0.17603316758531634\n",
            "train loss:0.26155808782858736\n",
            "train loss:0.1316663403380537\n",
            "train loss:0.2102943743288919\n",
            "train loss:0.2401211344742286\n",
            "train loss:0.17924684406278196\n",
            "train loss:0.2098063353441068\n",
            "train loss:0.1120566108246673\n",
            "train loss:0.2718607823490113\n",
            "train loss:0.13513305814067345\n",
            "train loss:0.2137660793847453\n",
            "train loss:0.14819060487126834\n",
            "train loss:0.08666563791856037\n",
            "train loss:0.2131922527556928\n",
            "train loss:0.2759248580258094\n",
            "train loss:0.15611099559666258\n",
            "train loss:0.15217162628785202\n",
            "train loss:0.21165805707610363\n",
            "train loss:0.1394547232632576\n",
            "train loss:0.20581874975458075\n",
            "train loss:0.2220424594928526\n",
            "train loss:0.09878444044069834\n",
            "train loss:0.21345664855051416\n",
            "train loss:0.11863303226597888\n",
            "train loss:0.10188325835614107\n",
            "train loss:0.2479449950496248\n",
            "train loss:0.2112274559494691\n",
            "train loss:0.2952578511997598\n",
            "train loss:0.17722436805045694\n",
            "=== epoch:7, train acc:0.928, test acc:0.926 ===\n",
            "train loss:0.16933639444007217\n",
            "train loss:0.21349009879675074\n",
            "train loss:0.20901338027269856\n",
            "train loss:0.17317192762374145\n",
            "train loss:0.1740864109446289\n",
            "train loss:0.18929619937881056\n",
            "train loss:0.18591965901563\n",
            "train loss:0.11605637668112358\n",
            "train loss:0.20725279081550232\n",
            "train loss:0.17312644722964537\n",
            "train loss:0.21356883793433032\n",
            "train loss:0.17858110941995547\n",
            "train loss:0.161687786995337\n",
            "train loss:0.07597622766859953\n",
            "train loss:0.14650148587423517\n",
            "train loss:0.12461728157261366\n",
            "train loss:0.21918001192528888\n",
            "train loss:0.17112635622644173\n",
            "train loss:0.12944110339558562\n",
            "train loss:0.08751004155906751\n",
            "train loss:0.1413966698014638\n",
            "train loss:0.1983144703527727\n",
            "train loss:0.10682261711066776\n",
            "train loss:0.11548090842983864\n",
            "train loss:0.13389056914271374\n",
            "train loss:0.13569035664862736\n",
            "train loss:0.1432864928959196\n",
            "train loss:0.18332707271260548\n",
            "train loss:0.1454658064589004\n",
            "train loss:0.09978950686719275\n",
            "train loss:0.11065645992162448\n",
            "train loss:0.12050547338754843\n",
            "train loss:0.13848052407296652\n",
            "train loss:0.17428741656897132\n",
            "train loss:0.1678292637996825\n",
            "train loss:0.158084980819805\n",
            "train loss:0.15047488727560013\n",
            "train loss:0.13263053437803624\n",
            "train loss:0.15573470421246408\n",
            "train loss:0.1909208632573652\n",
            "train loss:0.0811495637471345\n",
            "train loss:0.16062004886251166\n",
            "train loss:0.09426198292873275\n",
            "train loss:0.20320442779159642\n",
            "train loss:0.13319933582084287\n",
            "train loss:0.13802060405239724\n",
            "train loss:0.20310744507707124\n",
            "train loss:0.07480487718791379\n",
            "train loss:0.11478679845264236\n",
            "train loss:0.2096111638047647\n",
            "=== epoch:8, train acc:0.95, test acc:0.919 ===\n",
            "train loss:0.21946772152141605\n",
            "train loss:0.11517922538309752\n",
            "train loss:0.05312329758456175\n",
            "train loss:0.13102834526726967\n",
            "train loss:0.14974719313849788\n",
            "train loss:0.12074801819264387\n",
            "train loss:0.09692250889670401\n",
            "train loss:0.15908669913818907\n",
            "train loss:0.12503588643358635\n",
            "train loss:0.12330064450185142\n",
            "train loss:0.1573293002010478\n",
            "train loss:0.15150461640919677\n",
            "train loss:0.10945293942271364\n",
            "train loss:0.12377145841227012\n",
            "train loss:0.21511989862154818\n",
            "train loss:0.21458841532361864\n",
            "train loss:0.14555488436143016\n",
            "train loss:0.24705480943859584\n",
            "train loss:0.10205490778593203\n",
            "train loss:0.13278403912930353\n",
            "train loss:0.14945907107121076\n",
            "train loss:0.2709595404666162\n",
            "train loss:0.21488605718474452\n",
            "train loss:0.16021912594834548\n",
            "train loss:0.16642286312136162\n",
            "train loss:0.07293905688894993\n",
            "train loss:0.13124016973353292\n",
            "train loss:0.13372421097507875\n",
            "train loss:0.11942184748010483\n",
            "train loss:0.13974262876891774\n",
            "train loss:0.08741997649506067\n",
            "train loss:0.13434371599927947\n",
            "train loss:0.07910093489050359\n",
            "train loss:0.13095476299800193\n",
            "train loss:0.14384174698667015\n",
            "train loss:0.1309931965232913\n",
            "train loss:0.1324628105821825\n",
            "train loss:0.07622912925591017\n",
            "train loss:0.12228887009232998\n",
            "train loss:0.1125910138398008\n",
            "train loss:0.20558271202637496\n",
            "train loss:0.06329460925672159\n",
            "train loss:0.11820641036923493\n",
            "train loss:0.16048633028092774\n",
            "train loss:0.13802970247196666\n",
            "train loss:0.12330824667194079\n",
            "train loss:0.19571315659460442\n",
            "train loss:0.14475477671250134\n",
            "train loss:0.10978284135143478\n",
            "train loss:0.08403466434205561\n",
            "=== epoch:9, train acc:0.957, test acc:0.937 ===\n",
            "train loss:0.07354254969812364\n",
            "train loss:0.0944445828629939\n",
            "train loss:0.09844196739822011\n",
            "train loss:0.24185064649098184\n",
            "train loss:0.12983366823091133\n",
            "train loss:0.13454795320685536\n",
            "train loss:0.12744410494424402\n",
            "train loss:0.19754656356668632\n",
            "train loss:0.17050125605089847\n",
            "train loss:0.09392620179459613\n",
            "train loss:0.1302736621190314\n",
            "train loss:0.11858553445745054\n",
            "train loss:0.09072881881540519\n",
            "train loss:0.08596959367858799\n",
            "train loss:0.10887324903959573\n",
            "train loss:0.07413513748186211\n",
            "train loss:0.08832634643595158\n",
            "train loss:0.10609740888040926\n",
            "train loss:0.16411837513085137\n",
            "train loss:0.09961795737992109\n",
            "train loss:0.0781418272670336\n",
            "train loss:0.05804102555090812\n",
            "train loss:0.12524408342875035\n",
            "train loss:0.06814748504683923\n",
            "train loss:0.20282402139925318\n",
            "train loss:0.1378547986567704\n",
            "train loss:0.1357367757673031\n",
            "train loss:0.105945476817702\n",
            "train loss:0.1536276968909233\n",
            "train loss:0.07252945746512557\n",
            "train loss:0.06089574269203785\n",
            "train loss:0.09644955439939336\n",
            "train loss:0.10476137593114009\n",
            "train loss:0.16384708345290414\n",
            "train loss:0.06556065768453323\n",
            "train loss:0.1886228782933016\n",
            "train loss:0.12174902334283598\n",
            "train loss:0.05716610603141796\n",
            "train loss:0.153985710317957\n",
            "train loss:0.18213306391600934\n",
            "train loss:0.06612626681813177\n",
            "train loss:0.11142361858978242\n",
            "train loss:0.11476991122420763\n",
            "train loss:0.08798281181263343\n",
            "train loss:0.09073407002029882\n",
            "train loss:0.1311276370623055\n",
            "train loss:0.14589123052385497\n",
            "train loss:0.12515573743560546\n",
            "train loss:0.1462916565864106\n",
            "train loss:0.08158368958083036\n",
            "=== epoch:10, train acc:0.951, test acc:0.937 ===\n",
            "train loss:0.05792829990859261\n",
            "train loss:0.19339305407909405\n",
            "train loss:0.08868646591027379\n",
            "train loss:0.06940463114035673\n",
            "train loss:0.2026702802728822\n",
            "train loss:0.08011363041483294\n",
            "train loss:0.09310112445858129\n",
            "train loss:0.08821379900327779\n",
            "train loss:0.039378280569760074\n",
            "train loss:0.13156620845756145\n",
            "train loss:0.1585683361447534\n",
            "train loss:0.07631808070179112\n",
            "train loss:0.06114513843075248\n",
            "train loss:0.10907482014504741\n",
            "train loss:0.08198189101198962\n",
            "train loss:0.06408981612755374\n",
            "train loss:0.14925788763917253\n",
            "train loss:0.08701034379104972\n",
            "train loss:0.06917522026932826\n",
            "train loss:0.08769222691829437\n",
            "train loss:0.1273620805174265\n",
            "train loss:0.061929812592480656\n",
            "train loss:0.04896962240903095\n",
            "train loss:0.03845699221767678\n",
            "train loss:0.08335366097742886\n",
            "train loss:0.16827793169710326\n",
            "train loss:0.07059605335593068\n",
            "train loss:0.13891759193384737\n",
            "train loss:0.05871436565946006\n",
            "train loss:0.1407967674606285\n",
            "train loss:0.07693523992965992\n",
            "train loss:0.08940892268420594\n",
            "train loss:0.07295345813680738\n",
            "train loss:0.13676407692811\n",
            "train loss:0.1528020380872737\n",
            "train loss:0.10910590165613387\n",
            "train loss:0.10590967919895172\n",
            "train loss:0.068466504082043\n",
            "train loss:0.07767555120002269\n",
            "train loss:0.10821477349266033\n",
            "train loss:0.12049471077883611\n",
            "train loss:0.13544327610524215\n",
            "train loss:0.11951627372706591\n",
            "train loss:0.14937583176847422\n",
            "train loss:0.042573619512593426\n",
            "train loss:0.04917675234538674\n",
            "train loss:0.06516506998047063\n",
            "train loss:0.0630094583079461\n",
            "train loss:0.03678659419828751\n",
            "train loss:0.0975014815736806\n",
            "=== epoch:11, train acc:0.964, test acc:0.942 ===\n",
            "train loss:0.09067582150468802\n",
            "train loss:0.058668574404693835\n",
            "train loss:0.11595051399568171\n",
            "train loss:0.050617179039727865\n",
            "train loss:0.0495902325644488\n",
            "train loss:0.027234258373074374\n",
            "train loss:0.10190876560144611\n",
            "train loss:0.09714480915814015\n",
            "train loss:0.06459606327735928\n",
            "train loss:0.056147432476763674\n",
            "train loss:0.12316951210741206\n",
            "train loss:0.11465410412961564\n",
            "train loss:0.14369344232216408\n",
            "train loss:0.06471538170587843\n",
            "train loss:0.09881139746615533\n",
            "train loss:0.17265263841904377\n",
            "train loss:0.12053286280995809\n",
            "train loss:0.03915553022063275\n",
            "train loss:0.04822651526419872\n",
            "train loss:0.05164144009917167\n",
            "train loss:0.043185149376086605\n",
            "train loss:0.08869117648249315\n",
            "train loss:0.0803748617924076\n",
            "train loss:0.03651462800330687\n",
            "train loss:0.13463427121625027\n",
            "train loss:0.09591897527172603\n",
            "train loss:0.035095991360798405\n",
            "train loss:0.04130681766476707\n",
            "train loss:0.06518642817850875\n",
            "train loss:0.1384960963072666\n",
            "train loss:0.111817529130182\n",
            "train loss:0.0592799832583033\n",
            "train loss:0.08654408388042105\n",
            "train loss:0.051860715284955436\n",
            "train loss:0.04302649104348975\n",
            "train loss:0.0315800257964928\n",
            "train loss:0.21037883295951906\n",
            "train loss:0.08145965156395465\n",
            "train loss:0.08845729011364051\n",
            "train loss:0.07845018837114513\n",
            "train loss:0.08246773608742787\n",
            "train loss:0.13933278607474253\n",
            "train loss:0.046387193488070355\n",
            "train loss:0.071512700255342\n",
            "train loss:0.07477610099948934\n",
            "train loss:0.08579101775982945\n",
            "train loss:0.09093174256888555\n",
            "train loss:0.11811286413498477\n",
            "train loss:0.09486214288381306\n",
            "train loss:0.03934934578400331\n",
            "=== epoch:12, train acc:0.972, test acc:0.938 ===\n",
            "train loss:0.05144654241619767\n",
            "train loss:0.08217841197696972\n",
            "train loss:0.06011861395076891\n",
            "train loss:0.06414492337762018\n",
            "train loss:0.13843312897688959\n",
            "train loss:0.058813726552946\n",
            "train loss:0.05200050130892133\n",
            "train loss:0.05153164701018958\n",
            "train loss:0.09713151144075863\n",
            "train loss:0.0709901374204795\n",
            "train loss:0.043942869401333434\n",
            "train loss:0.08494168541851888\n",
            "train loss:0.04685664708241294\n",
            "train loss:0.05093507006089032\n",
            "train loss:0.0746399717529847\n",
            "train loss:0.06404523324538512\n",
            "train loss:0.04445542424295029\n",
            "train loss:0.10149370616650794\n",
            "train loss:0.06441780878545267\n",
            "train loss:0.05475365312020517\n",
            "train loss:0.05313003090756876\n",
            "train loss:0.04013203479446156\n",
            "train loss:0.07025051729931056\n",
            "train loss:0.074605941307906\n",
            "train loss:0.09765664663098253\n",
            "train loss:0.040645553127445916\n",
            "train loss:0.026288580669286313\n",
            "train loss:0.04277791957722608\n",
            "train loss:0.06328199359408923\n",
            "train loss:0.05741814852334281\n",
            "train loss:0.09011695972459394\n",
            "train loss:0.05295203960643249\n",
            "train loss:0.05650589969314837\n",
            "train loss:0.11056070677509404\n",
            "train loss:0.09684491701277363\n",
            "train loss:0.02987161535749164\n",
            "train loss:0.049080104674201014\n",
            "train loss:0.05495790021036609\n",
            "train loss:0.03898702798862502\n",
            "train loss:0.06279435285387441\n",
            "train loss:0.14548769079094007\n",
            "train loss:0.11821040836800806\n",
            "train loss:0.041393374759097934\n",
            "train loss:0.09977717370282607\n",
            "train loss:0.0972095561331408\n",
            "train loss:0.04973006865655799\n",
            "train loss:0.06047671642874814\n",
            "train loss:0.12420213040895874\n",
            "train loss:0.06641291763101724\n",
            "train loss:0.10770291938713354\n",
            "=== epoch:13, train acc:0.968, test acc:0.943 ===\n",
            "train loss:0.08992200790395474\n",
            "train loss:0.0754872710547065\n",
            "train loss:0.03627450151204293\n",
            "train loss:0.0686995064496222\n",
            "train loss:0.04830588865845248\n",
            "train loss:0.06197149880092695\n",
            "train loss:0.03758332543368645\n",
            "train loss:0.12777680527341295\n",
            "train loss:0.08511133410586481\n",
            "train loss:0.08503578791991066\n",
            "train loss:0.09167239359497259\n",
            "train loss:0.14517460501456375\n",
            "train loss:0.05576553875449486\n",
            "train loss:0.12554635546691612\n",
            "train loss:0.10378306447631327\n",
            "train loss:0.1542813146437196\n",
            "train loss:0.1253412629794035\n",
            "train loss:0.061922767727577155\n",
            "train loss:0.0478435080586263\n",
            "train loss:0.059527298353809315\n",
            "train loss:0.07536525926223897\n",
            "train loss:0.07372274458480806\n",
            "train loss:0.038679637263691065\n",
            "train loss:0.031378887320674835\n",
            "train loss:0.08495856084149485\n",
            "train loss:0.1547339982156412\n",
            "train loss:0.04285649627005644\n",
            "train loss:0.04738271404004755\n",
            "train loss:0.04763770265772123\n",
            "train loss:0.06676113250947611\n",
            "train loss:0.061265207077712824\n",
            "train loss:0.041122100284061415\n",
            "train loss:0.04086119288298449\n",
            "train loss:0.06445507317374898\n",
            "train loss:0.0475533531362871\n",
            "train loss:0.051953764536053146\n",
            "train loss:0.01923315215645357\n",
            "train loss:0.06456604086995357\n",
            "train loss:0.08501671776709975\n",
            "train loss:0.05332963600403379\n",
            "train loss:0.04050385635340996\n",
            "train loss:0.0690677007875902\n",
            "train loss:0.02767625001194309\n",
            "train loss:0.061001299737917486\n",
            "train loss:0.0764164740342979\n",
            "train loss:0.09975724110026869\n",
            "train loss:0.05012441308214679\n",
            "train loss:0.06158370098918449\n",
            "train loss:0.03693201518692987\n",
            "train loss:0.03371084894049667\n",
            "=== epoch:14, train acc:0.977, test acc:0.949 ===\n",
            "train loss:0.046795531265062466\n",
            "train loss:0.08808197107581764\n",
            "train loss:0.02972964479674689\n",
            "train loss:0.04770326667009784\n",
            "train loss:0.030261160719384974\n",
            "train loss:0.02499502446588505\n",
            "train loss:0.0512198858225559\n",
            "train loss:0.05120951861377913\n",
            "train loss:0.04019764268906986\n",
            "train loss:0.050501918965964815\n",
            "train loss:0.05072478621430129\n",
            "train loss:0.09778950184239193\n",
            "train loss:0.02481826840711423\n",
            "train loss:0.03346586259570008\n",
            "train loss:0.03969529695540828\n",
            "train loss:0.06152620379888241\n",
            "train loss:0.03127113684403665\n",
            "train loss:0.053386855753119286\n",
            "train loss:0.062291485857565554\n",
            "train loss:0.045822529606034036\n",
            "train loss:0.17272639966810324\n",
            "train loss:0.10902357478673853\n",
            "train loss:0.08600306875120416\n",
            "train loss:0.04354746112712158\n",
            "train loss:0.07547377645621312\n",
            "train loss:0.0740388410588511\n",
            "train loss:0.022456157347064836\n",
            "train loss:0.09239236233372945\n",
            "train loss:0.1078938347326721\n",
            "train loss:0.08725462004439631\n",
            "train loss:0.026561529831078855\n",
            "train loss:0.024109095365845955\n",
            "train loss:0.025421311706670374\n",
            "train loss:0.06498175017191217\n",
            "train loss:0.07755417551274527\n",
            "train loss:0.047943692338734\n",
            "train loss:0.03863557883928317\n",
            "train loss:0.053767026438733705\n",
            "train loss:0.04206968997392823\n",
            "train loss:0.03561503339894313\n",
            "train loss:0.12965601623862053\n",
            "train loss:0.019680232766550693\n",
            "train loss:0.022670648872181117\n",
            "train loss:0.03561231331898766\n",
            "train loss:0.07076135164907064\n",
            "train loss:0.04479634824835916\n",
            "train loss:0.031540805535365445\n",
            "train loss:0.043718629548220075\n",
            "train loss:0.031796807884173495\n",
            "train loss:0.07592637039433034\n",
            "=== epoch:15, train acc:0.981, test acc:0.957 ===\n",
            "train loss:0.0302655521069649\n",
            "train loss:0.024966633296636825\n",
            "train loss:0.06484172790163754\n",
            "train loss:0.023567487462131106\n",
            "train loss:0.030954029561961218\n",
            "train loss:0.13074710012060448\n",
            "train loss:0.025160775170661728\n",
            "train loss:0.046512227040150664\n",
            "train loss:0.04301648033890798\n",
            "train loss:0.042700665203648604\n",
            "train loss:0.03537555853036318\n",
            "train loss:0.04281715417888871\n",
            "train loss:0.04443468055862282\n",
            "train loss:0.10207604874676568\n",
            "train loss:0.056670257074153546\n",
            "train loss:0.030223716359830347\n",
            "train loss:0.047047478364538715\n",
            "train loss:0.024786925663131884\n",
            "train loss:0.05025546807664557\n",
            "train loss:0.03820519552491682\n",
            "train loss:0.06459833130256953\n",
            "train loss:0.01896645239025631\n",
            "train loss:0.09219440407708683\n",
            "train loss:0.02323532121221144\n",
            "train loss:0.05825946338450059\n",
            "train loss:0.09253490612155849\n",
            "train loss:0.029599274773623972\n",
            "train loss:0.046411618395439655\n",
            "train loss:0.0795678248178272\n",
            "train loss:0.06574051534081665\n",
            "train loss:0.02110474494848648\n",
            "train loss:0.06475023797949322\n",
            "train loss:0.06208205136687228\n",
            "train loss:0.018459459317223335\n",
            "train loss:0.06135889754589164\n",
            "train loss:0.04612932253754295\n",
            "train loss:0.03986943141244187\n",
            "train loss:0.04506193965901521\n",
            "train loss:0.07647428778334514\n",
            "train loss:0.034524883774075026\n",
            "train loss:0.125783906174786\n",
            "train loss:0.034124255679104216\n",
            "train loss:0.04562547570877862\n",
            "train loss:0.06262167471752421\n",
            "train loss:0.037261651229644775\n",
            "train loss:0.09969834355534302\n",
            "train loss:0.07003638988041697\n",
            "train loss:0.1318588184904118\n",
            "train loss:0.07255258108090301\n",
            "train loss:0.047964365940204756\n",
            "=== epoch:16, train acc:0.98, test acc:0.948 ===\n",
            "train loss:0.11676474790461429\n",
            "train loss:0.04204763883832955\n",
            "train loss:0.08799433300969396\n",
            "train loss:0.04625431097272305\n",
            "train loss:0.10196465300506043\n",
            "train loss:0.029126927249008117\n",
            "train loss:0.05933462211105245\n",
            "train loss:0.029608968937538158\n",
            "train loss:0.07037912132554978\n",
            "train loss:0.04467766466237205\n",
            "train loss:0.058266626741444706\n",
            "train loss:0.025054515991865686\n",
            "train loss:0.0336930833185961\n",
            "train loss:0.07300706504752597\n",
            "train loss:0.036088570211605914\n",
            "train loss:0.052382309094934625\n",
            "train loss:0.04485644468570481\n",
            "train loss:0.050458826066646416\n",
            "train loss:0.059436174486842866\n",
            "train loss:0.0767040404791431\n",
            "train loss:0.021295075302628915\n",
            "train loss:0.01926923940321696\n",
            "train loss:0.03801626947592724\n",
            "train loss:0.04253539064157128\n",
            "train loss:0.029472360813026618\n",
            "train loss:0.06391917216786928\n",
            "train loss:0.08275820350500883\n",
            "train loss:0.03491189764217673\n",
            "train loss:0.01538560569674204\n",
            "train loss:0.0395435594637018\n",
            "train loss:0.028896241221118876\n",
            "train loss:0.06705483909889884\n",
            "train loss:0.034219914098133415\n",
            "train loss:0.016924850899070552\n",
            "train loss:0.057607495971183455\n",
            "train loss:0.016455046181017584\n",
            "train loss:0.03931807593898857\n",
            "train loss:0.044335775261981444\n",
            "train loss:0.062895223962841\n",
            "train loss:0.03840401048116779\n",
            "train loss:0.05852613946380554\n",
            "train loss:0.016571003820163674\n",
            "train loss:0.026656882449997473\n",
            "train loss:0.04545891215070791\n",
            "train loss:0.014601659769663265\n",
            "train loss:0.04568330968844154\n",
            "train loss:0.06181349336759131\n",
            "train loss:0.029920633289835616\n",
            "train loss:0.01878644168220763\n",
            "train loss:0.04658778996764711\n",
            "=== epoch:17, train acc:0.986, test acc:0.955 ===\n",
            "train loss:0.0280264044362088\n",
            "train loss:0.011750563815709016\n",
            "train loss:0.039306566412403494\n",
            "train loss:0.01806800281171282\n",
            "train loss:0.024276721264145117\n",
            "train loss:0.05173574032547188\n",
            "train loss:0.020029525929780034\n",
            "train loss:0.032574818124917145\n",
            "train loss:0.024311816102691265\n",
            "train loss:0.006653337983967293\n",
            "train loss:0.07695216257206317\n",
            "train loss:0.023191809460082036\n",
            "train loss:0.01731292372528299\n",
            "train loss:0.0446469738100555\n",
            "train loss:0.04151564790846626\n",
            "train loss:0.03324421097169789\n",
            "train loss:0.07458530371714109\n",
            "train loss:0.04796824686423184\n",
            "train loss:0.02801491176327708\n",
            "train loss:0.017368782418827233\n",
            "train loss:0.10516675269827949\n",
            "train loss:0.029856883791779438\n",
            "train loss:0.026913730573592507\n",
            "train loss:0.029373746627118155\n",
            "train loss:0.02122162925757241\n",
            "train loss:0.04597564892008494\n",
            "train loss:0.03536227198384124\n",
            "train loss:0.02004439484522701\n",
            "train loss:0.08411501123295545\n",
            "train loss:0.06727432361470212\n",
            "train loss:0.0257205696711135\n",
            "train loss:0.06672927110753771\n",
            "train loss:0.020630787008883655\n",
            "train loss:0.06769482076414401\n",
            "train loss:0.04788754173463615\n",
            "train loss:0.022604252403637393\n",
            "train loss:0.014956283387135725\n",
            "train loss:0.033141925919723085\n",
            "train loss:0.06380697410429563\n",
            "train loss:0.013488089786767514\n",
            "train loss:0.04285354192603078\n",
            "train loss:0.05240394152359755\n",
            "train loss:0.03702341580600356\n",
            "train loss:0.05158699032774502\n",
            "train loss:0.017054294765343404\n",
            "train loss:0.0181149288078334\n",
            "train loss:0.04793861527026094\n",
            "train loss:0.037462456993761495\n",
            "train loss:0.022469283301135446\n",
            "train loss:0.02377842025450015\n",
            "=== epoch:18, train acc:0.99, test acc:0.962 ===\n",
            "train loss:0.008149614878215727\n",
            "train loss:0.035046106136908714\n",
            "train loss:0.018391031573812916\n",
            "train loss:0.04229932629119083\n",
            "train loss:0.02601857939675302\n",
            "train loss:0.05911422315276088\n",
            "train loss:0.02029806833078804\n",
            "train loss:0.09218958432046848\n",
            "train loss:0.06368948925122095\n",
            "train loss:0.03223172746614668\n",
            "train loss:0.016256943391201692\n",
            "train loss:0.027685980995013338\n",
            "train loss:0.028641562556796587\n",
            "train loss:0.029379774838997456\n",
            "train loss:0.04517677774628899\n",
            "train loss:0.020566219963862476\n",
            "train loss:0.03552867616425989\n",
            "train loss:0.042604092635859514\n",
            "train loss:0.02396125166690488\n",
            "train loss:0.010507630653212056\n",
            "train loss:0.024348440610692933\n",
            "train loss:0.03776167664536941\n",
            "train loss:0.012820300577701853\n",
            "train loss:0.004801319234777756\n",
            "train loss:0.010669377279246169\n",
            "train loss:0.0363773646780681\n",
            "train loss:0.03274632979666215\n",
            "train loss:0.050643601788815225\n",
            "train loss:0.008198605808097363\n",
            "train loss:0.1215153138006034\n",
            "train loss:0.02608353516915923\n",
            "train loss:0.052505718650103984\n",
            "train loss:0.020536217897279982\n",
            "train loss:0.024289114815619815\n",
            "train loss:0.018971427513267865\n",
            "train loss:0.021328139234973346\n",
            "train loss:0.03834383344806987\n",
            "train loss:0.030090002867783815\n",
            "train loss:0.04711841135009448\n",
            "train loss:0.014879266646247433\n",
            "train loss:0.026009344191216446\n",
            "train loss:0.04327462741363572\n",
            "train loss:0.023173640499724667\n",
            "train loss:0.042803484781635176\n",
            "train loss:0.021022693079982623\n",
            "train loss:0.038269987224526195\n",
            "train loss:0.028924924659235553\n",
            "train loss:0.02001389849567971\n",
            "train loss:0.033117638772391905\n",
            "train loss:0.04383420538663946\n",
            "=== epoch:19, train acc:0.986, test acc:0.959 ===\n",
            "train loss:0.028123537069946726\n",
            "train loss:0.015222328833092427\n",
            "train loss:0.018756214267774497\n",
            "train loss:0.10185742795816723\n",
            "train loss:0.049184957514828716\n",
            "train loss:0.020425350559496833\n",
            "train loss:0.03847411814858076\n",
            "train loss:0.017681654472605664\n",
            "train loss:0.015709890191560338\n",
            "train loss:0.012461702287301979\n",
            "train loss:0.02821710610058453\n",
            "train loss:0.07107641909060305\n",
            "train loss:0.01822215891873609\n",
            "train loss:0.06521390356584324\n",
            "train loss:0.023114197839533845\n",
            "train loss:0.0171188165102917\n",
            "train loss:0.015602954857216593\n",
            "train loss:0.038257999572223755\n",
            "train loss:0.05719406091378647\n",
            "train loss:0.023279732588839547\n",
            "train loss:0.014204163923031723\n",
            "train loss:0.014103834297137432\n",
            "train loss:0.04371932306765978\n",
            "train loss:0.02511635743859543\n",
            "train loss:0.04363670468517562\n",
            "train loss:0.011628356375956191\n",
            "train loss:0.028165450196793104\n",
            "train loss:0.02817940685764737\n",
            "train loss:0.021868797877093123\n",
            "train loss:0.0449126057056916\n",
            "train loss:0.011414646532438876\n",
            "train loss:0.011470017347798366\n",
            "train loss:0.013094366569950255\n",
            "train loss:0.040269650603956195\n",
            "train loss:0.01865189909668925\n",
            "train loss:0.012248152785850196\n",
            "train loss:0.04626356557127899\n",
            "train loss:0.02923593446978791\n",
            "train loss:0.048437925479354965\n",
            "train loss:0.018845003836409135\n",
            "train loss:0.028312544736735227\n",
            "train loss:0.02299523952039388\n",
            "train loss:0.016889801605592357\n",
            "train loss:0.043975391302766634\n",
            "train loss:0.01641831633436275\n",
            "train loss:0.022732679848078897\n",
            "train loss:0.020828140912278314\n",
            "train loss:0.07432722440005762\n",
            "train loss:0.008596806640067177\n",
            "train loss:0.01651375395656677\n",
            "=== epoch:20, train acc:0.992, test acc:0.958 ===\n",
            "train loss:0.030887029227776766\n",
            "train loss:0.005900733021010566\n",
            "train loss:0.024035593581552737\n",
            "train loss:0.01661046532192034\n",
            "train loss:0.041091985705465676\n",
            "train loss:0.011603602983145327\n",
            "train loss:0.024464137172569848\n",
            "train loss:0.012799837956452224\n",
            "train loss:0.027720930831529964\n",
            "train loss:0.014997816524102613\n",
            "train loss:0.027120593238529233\n",
            "train loss:0.02521473917360336\n",
            "train loss:0.006325372878865386\n",
            "train loss:0.027936043758035423\n",
            "train loss:0.014782917970293139\n",
            "train loss:0.017336065853428197\n",
            "train loss:0.014587989494035582\n",
            "train loss:0.021857683869552936\n",
            "train loss:0.04007432807630185\n",
            "train loss:0.039977554073286015\n",
            "train loss:0.02504381112017258\n",
            "train loss:0.022432706107178787\n",
            "train loss:0.0177618314533363\n",
            "train loss:0.01466791206526294\n",
            "train loss:0.006560819382642757\n",
            "train loss:0.02972405500445779\n",
            "train loss:0.019607228750699902\n",
            "train loss:0.00820948228924487\n",
            "train loss:0.02495947959504174\n",
            "train loss:0.018735628021113174\n",
            "train loss:0.008900475782656778\n",
            "train loss:0.01699593796293529\n",
            "train loss:0.012066288911421245\n",
            "train loss:0.018360156436375952\n",
            "train loss:0.026426508099257003\n",
            "train loss:0.009616707098924265\n",
            "train loss:0.03403593706153779\n",
            "train loss:0.018412837589281476\n",
            "train loss:0.012439069894799211\n",
            "train loss:0.010533675358845489\n",
            "train loss:0.01167313688102329\n",
            "train loss:0.04771969363267143\n",
            "train loss:0.02247317952437148\n",
            "train loss:0.05111539432731425\n",
            "train loss:0.0368172710616782\n",
            "train loss:0.017218631718886324\n",
            "train loss:0.017729703191965\n",
            "train loss:0.013007003356072666\n",
            "train loss:0.017695126089034983\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.953\n",
            "Saved Network Parameters!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cnn confusion matxi\n",
        "# coding: utf-8\n",
        "import sys, os\n",
        "# sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from dataset.mnist import load_mnist\n",
        "from common.simple_convnet import SimpleConvNet\n",
        "np.set_printoptions(linewidth=150,threshold=1000)\n",
        "\n",
        "def get_data():\n",
        "    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=False, one_hot_label=False)\n",
        "    return x_test, t_test\n",
        "\n",
        "x, t = get_data()\n",
        "\n",
        "network = SimpleConvNet()\n",
        "\n",
        "# 학습된 가중치\n",
        "network.load_params(\"params.pkl\")\n",
        "\n",
        "confusion = np.zeros((10,10), dtype=int)\n",
        "\n",
        "for k in range(len(x)):\n",
        "    i=int(t[k])\n",
        "    y = network.predict(x[k].reshape(1,*x[k].shape))\n",
        "    j= np.argmax(y) # 확률이 가장 높은 원소의 인덱스를 얻는다.\n",
        "    confusion[i][j] += 1\n",
        "\n",
        "print(confusion)\n",
        "\n",
        "accuracy=0\n",
        "\n",
        "for k in range(len(confusion)):\n",
        "    accuracy+=confusion[k][k]\n",
        "\n",
        "accuracy=accuracy/np.sum(confusion)*100\n",
        "print(accuracy)\n",
        "\n",
        "y = np.argmax(network.predict(x), axis=1)\n",
        "\n",
        "number1=0\n",
        "number2=6\n",
        "number1_number2=[]\n",
        "number2_number1=[]\n",
        "\n",
        "for k in range(len(x)):\n",
        "    i=int(t[k])\n",
        "    y = network.predict(x[k].reshape(1,*x[k].shape))\n",
        "    j= np.argmax(y) # 확률이 가장 높은 원소의 인덱스를 얻는다.\n",
        "    if i==number1 and j==number2:\n",
        "        number1_number2.append(k)\n",
        "    if i==number2 and j==number1:\n",
        "        number2_number1.append(k)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(len(number1_number2)):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(x[number1_number2[i]].reshape(28,28), cmap=plt.cm.binary)\n",
        "    plt.xlabel(t[number1_number2[i]])\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(len(number2_number1)):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(x[number2_number1[i]].reshape(28,28), cmap=plt.cm.binary)\n",
        "    plt.xlabel(t[number2_number1[i]])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 881
        },
        "id": "YH6wJuQojUBR",
        "outputId": "d3c3c83a-e4c7-44be-9595-541409ad1363"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 961    0    2    0    0    3    8    3    3    0]\n",
            " [   0 1123    4    1    0    1    3    0    3    0]\n",
            " [   4    3  996   11    3    0    1    7    7    0]\n",
            " [   0    1    6  977    0   10    0    5    8    3]\n",
            " [   0    4    2    0  959    0   10    1    2    4]\n",
            " [   4    2    0    3    0  869    3    2    9    0]\n",
            " [   7    3    5    0   10   14  914    0    5    0]\n",
            " [   0    8   24    3    4    1    0  972    3   13]\n",
            " [   3    5    6   21    6   15    8    6  902    2]\n",
            " [   3   10    2   11   28   15    1   12   12  915]]\n",
            "95.88\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 7 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}